{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c8767d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['address', 'name', 'isContract', 'isWhale', 'network', 'serviceType',\n",
       "       'sub1Service', 'walletPurpose', 'sub1Wallet'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "addrs = pd.read_csv('addr_info.csv', index_col=0)\n",
    "addrs = addrs[~((addrs.duplicated(subset=['address'])) & \n",
    "          (~addrs['name'].isin(['binance', 'coinbase', 'bullish'])))].drop_duplicates(subset=['address'])\n",
    "addrs.name = addrs.name.str.lower()\n",
    "feats = ['name', 'serviceType', 'sub1Service', 'walletPurpose', 'sub1Wallet']\n",
    "addrs.columns\n",
    "# addrs[addrs.address.isin(addrs[addrs.duplicated(subset=['address'])].address)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced7fead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name      serviceType  sub1Service  walletPurpose  sub1Wallet\n",
       "binance   CeFi         exc          Vault          deposit       11341\n",
       "coinbase  CeFi         exc          Vault          deposit        6188\n",
       "binance   CeFi         exc          Vault          hot              24\n",
       "                       fund         Vault          deposit          20\n",
       "bullish   CeFi         exc          Vault          deposit          11\n",
       "coinbase  CeFi         exc          Vault          hot               8\n",
       "                       fund         Vault          deposit           7\n",
       "                       exc          Vault          cold              6\n",
       "binance   CeFi         custody      Vault          deposit           1\n",
       "bullish   CeFi         exc          Vault          hot               1\n",
       "coinbase  CeFi         exc          Vault          proxy             1\n",
       "          DeFi         exc          Vault          proxy             1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addrs.loc[addrs.name.isin(['binance', 'coinbase', 'bullish'])].value_counts(['name', 'serviceType', 'sub1Service', 'walletPurpose', 'sub1Wallet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdee9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB connection established successfully\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "try:\n",
    "    conn = duckdb.connect('duckdb_test.db', read_only=False)\n",
    "    print('DuckDB connection established successfully')\n",
    "except Exception as e:\n",
    "    print(f'Error connecting to DuckDB: {str(e)}')\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90955ddb",
   "metadata": {},
   "source": [
    "# ETH Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ddc7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = dict(\n",
    "    tether='0xdAC17F958D2ee523a2206206994597C13D831ec7',\n",
    "    usdc='0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48',\n",
    "    dai='0x6B175474E89094C44Da98b954EedeAC495271d0F',\n",
    "    fdusd='0xc5f0f7b66764F6ec8C8Dff7BA683102295E16409',\n",
    "    weth='0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2',\n",
    "    wbtc='0x2260FAC5E5542a773Aa44fBCfeDf7C193bc2C599',\n",
    "    bnb='0xB8c77482e45F1F44dE1745F52C74426C631bDD52',\n",
    "    ada='0x3ee2200efb3400fabb9aacf31297cbdd1d435d47',\n",
    "    link='0x514910771af9ca656af840dff83e8264ecf986ca',\n",
    "    aethusdt='0x23878914EFE38d27C4D67Ab83ed1b93A74D4086a',\n",
    "    cusdt='0xf650C3d88D12dB855b8bf7D11Be6C55A4e07dCC9'\n",
    "\n",
    ")\n",
    "tokens = {k: v.lower() for k, v in tokens.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08ef43c",
   "metadata": {},
   "source": [
    "### Grap Addresses on each month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb8cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2024,2025):\n",
    "    for month in range(7, 11):\n",
    "        # Create the table\n",
    "        df_martial_law = conn.execute(f\"\"\"\n",
    "            SELECT *\n",
    "            FROM read_parquet('ethereum_token_transfer/*/*/*.parquet', hive_partitioning = true)\n",
    "            WHERE year={year} AND month={month}\n",
    "        \"\"\").df()\n",
    "\n",
    "        for k, v in tokens.items():\n",
    "            df_martial_law.loc[df_martial_law.token_address==v, 'token_name'] = k\n",
    "            \n",
    "        for v in ['token', 'from', 'to']:\n",
    "            df_martial_law = df_martial_law.merge(\n",
    "            addrs[['address']+feats], \n",
    "            left_on=f'{v}_address', right_on='address', \n",
    "            how=\"left\", validate='m:1'\n",
    "        ).rename(columns={c:f'{v}_{c}' for c in feats}).drop(columns=['address'])\n",
    "        df_martial_law['token_name'] = df_martial_law.filter(like='token_name').bfill(axis=1).iloc[:, 0]\n",
    "        df_martial_law = df_martial_law.loc[:, ~df_martial_law.columns.duplicated()]\n",
    "\n",
    "        df_ml_graph = df_martial_law[[\n",
    "            'from_name', 'to_name', 'transaction_hash', 'raw_amount', 'block_timestamp', 'from_address', 'to_address'\n",
    "        ]].copy().reset_index(drop=True)\n",
    "        for v in ['from', 'to']:\n",
    "            df_ml_graph.loc[df_ml_graph[f'{v}_name'].isna(), f'{v}_name'] =  df_ml_graph.loc[df_ml_graph[f'{v}_name'].isna(), f'{v}_address' ]\n",
    "        \n",
    "\n",
    "        df_ml_graph = df_ml_graph.drop(columns=['from_address', 'to_address'])\n",
    "        sims = pd.concat([\n",
    "            df_ml_graph.from_name.value_counts().reset_index().rename(columns={'from_name': 'name'}),\n",
    "            df_ml_graph.to_name.value_counts().reset_index().rename(columns={'to_name': 'name'})])\n",
    "        simss = sims.groupby('name').sum('count')\n",
    "        simss['year'] = year\n",
    "        simss['month'] = month\n",
    "        simss.to_csv(f'meta/{year}_{month}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c973063c",
   "metadata": {},
   "source": [
    "### Save Labelled Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98b50d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbb0c5ec72e4d18b0b692817fe1ce59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae9d161faec4b89a3ca2698814b3cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d8745050e8423bae4927b5ebe8ab24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56992f60feb44fd88a48353dcb7973e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2081bede9af2414f94adb9a2300dd807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51c57ae4801493c9be6449fbd38a2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b5601ca09b4a1aab1007c6c89a34ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_tokens = [\n",
    "    'tether', 'usdc', #'tethertreasury', 'circle'\n",
    "]\n",
    "\n",
    "# 603 minutes\n",
    "\n",
    "# for year in range(2016,2025):\n",
    "# for year in [2025]:\n",
    "    # for month in range(1, 6):/\n",
    "# for year, month in [(2025, 5), (2015, 12), (2015, 11), (2015, 10)]:\n",
    "for year, month in [(2025, i) for i in range(1, 4)] + [(2024, i) for i in range(7, 11)]:\n",
    "        # Create the table\n",
    "        df_martial_law = conn.execute(f\"\"\"\n",
    "            SELECT *\n",
    "            FROM read_parquet('ethereum_token_transfer/*/*/*.parquet', hive_partitioning = true)\n",
    "            WHERE year={year} AND month={month}\n",
    "        \"\"\").df()\n",
    "\n",
    "        for k, v in tokens.items():\n",
    "            if k not in target_tokens:\n",
    "                continue\n",
    "            df_martial_law.loc[df_martial_law.token_address==v, 'token_name'] = k\n",
    "            \n",
    "        for v in ['token', 'from', 'to']:\n",
    "            df_martial_law = df_martial_law.merge(\n",
    "            addrs[['address']+feats], \n",
    "            left_on=f'{v}_address', right_on='address', \n",
    "            how=\"left\", validate='m:1'\n",
    "        ).rename(columns={c:f'{v}_{c}' for c in feats}).drop(columns=['address'])\n",
    "        df_martial_law['token_name'] = df_martial_law.filter(like='token_name').bfill(axis=1).iloc[:, 0]\n",
    "        df_martial_law = df_martial_law.loc[:, ~df_martial_law.columns.duplicated()]\n",
    "        df_ml = df_martial_law.reset_index(drop=True)\n",
    "        # for v in ['from', 'to']:\n",
    "        #     df_ml.loc[df_ml[f'{v}_name'].isna(), f'{v}_name'] =  df_ml.loc[df_ml[f'{v}_name'].isna(), f'{v}_address' ]\n",
    "    \n",
    "        df_ml = df_ml.loc[~(df_ml.from_name.isna() & df_ml.to_name.isna())]\n",
    "        df_ml.loc[df_ml.from_name.isna(), 'from_name'] = 'idk'\n",
    "        df_ml.loc[df_ml.to_name.isna(), 'to_name'] = 'idk'\n",
    "        df_ml = df_ml.drop(columns=['from_address', 'to_address', 'token_address'])\n",
    "        df_ml.loc[df_ml.token_name.isin(target_tokens)].to_parquet(\n",
    "            f'labelled/eth_martial_law_{year}_{month:02d}.parquet',\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cdad08",
   "metadata": {},
   "source": [
    "# TRON Newtork\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f23562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.makedirs('meta_tron', exist_ok=True)\n",
    "\n",
    "tokens = dict(\n",
    "    tether='TR7NHqjeKQxGTCi8q8ZY4pL8otSzgjLj6t',\n",
    "    # studst='TThzxNRLrW2Brp9DcTQU8i4Wd9udCWEdZ3',\n",
    "    # usdc='TEkxiTehnzSmSe2XqrBj4w32RUN966rdz8',\n",
    "    # weth='T9d1b7c8e2f3c4d5e6f7a8b9c0d1e2f3g4h5i6j7n',\n",
    ")\n",
    "tokens = {k: v.lower() for k, v in tokens.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d98781",
   "metadata": {},
   "source": [
    "### Grep Addresses on each month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb896dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for year in [2024]: # range(2024,2026):\n",
    "    for month in range(10, 13):\n",
    "        # Create the table\n",
    "        df_martial_law = conn.execute(f\"\"\"\n",
    "            SELECT *\n",
    "            FROM read_parquet('tron_token_transfer/*/*/*.parquet', hive_partitioning = true)\n",
    "            WHERE year={year} AND month={month}\n",
    "        \"\"\").df()\n",
    "        if df_martial_law.empty:\n",
    "            print(f\"No data for {year}-{month}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        for k, v in tokens.items():\n",
    "            df_martial_law.loc[df_martial_law.token_address==v, 'token_name'] = k\n",
    "            \n",
    "        for v in ['token', 'from', 'to']:\n",
    "            df_martial_law = df_martial_law.merge(\n",
    "            addrs[['address']+feats], \n",
    "            left_on=f'{v}_address', right_on='address', \n",
    "            how=\"left\", validate='m:1'\n",
    "        ).rename(columns={c:f'{v}_{c}' for c in feats}).drop(columns=['address'])\n",
    "        df_martial_law['token_name'] = df_martial_law.filter(like='token_name').bfill(axis=1).iloc[:, 0]\n",
    "        df_martial_law = df_martial_law.loc[:, ~df_martial_law.columns.duplicated()]\n",
    "\n",
    "\n",
    "\n",
    "        df_ml_graph = df_martial_law[[\n",
    "            'from_name', 'to_name', 'transaction_hash', 'raw_amount', 'block_timestamp', 'from_address', 'to_address'\n",
    "        ]].copy().reset_index(drop=True)\n",
    "        for v in ['from', 'to']:\n",
    "            df_ml_graph.loc[df_ml_graph[f'{v}_name'].isna(), f'{v}_name'] =  df_ml_graph.loc[df_ml_graph[f'{v}_name'].isna(), f'{v}_address' ]\n",
    "        \n",
    "\n",
    "        df_ml_graph = df_ml_graph.drop(columns=['from_address', 'to_address'])\n",
    "        sims = pd.concat([\n",
    "            df_ml_graph.from_name.value_counts().reset_index().rename(columns={'from_name': 'name'}),\n",
    "            df_ml_graph.to_name.value_counts().reset_index().rename(columns={'to_name': 'name'})])\n",
    "        simss = sims.groupby('name').sum('count')\n",
    "        simss['year'] = year\n",
    "        simss['month'] = month\n",
    "        simss.to_csv(f'meta_tron/{year}_{month}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bc7f98",
   "metadata": {},
   "source": [
    "### Save Labelled Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef2551c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading data for 2023-5: Invalid Input Error: No magic bytes found at end of file 'tron_token_transfer\\year=2023\\month=5\\data_0.parquet'\n",
      "Error reading data for 2023-6: Invalid Input Error: No magic bytes found at end of file 'tron_token_transfer\\year=2023\\month=6\\data_0.parquet'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37db3aefabc4ed3b114f4236e17f20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b212b6329aa648709e33d89953d93ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# for year, month in [(y, m) for y in range(2020, 2025) for m in range(1, 13)] + [\n",
    "#     (2025, i) for i in range(1, 6)] + [(2019, i) for i in range(7, 13)]:\n",
    "for year, month in \n",
    "# /[(2023, i) for i in range(5, 7)] + [(2024, i) for i in range(11, 13)]:\n",
    "        try:\n",
    "            df = conn.execute(f\"\"\"\n",
    "                SELECT *\n",
    "                FROM read_parquet('tron_token_transfer/*/*/*.parquet', hive_partitioning = true)\n",
    "                WHERE year={year} AND month={month}\n",
    "            \"\"\").df()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading data for {year}-{month}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        df.from_address = df.from_address.str.lower()\n",
    "        df.to_address = df.to_address.str.lower()\n",
    "        df.token_address = df.token_address.str.lower()\n",
    "        \n",
    "        df = df.loc[df.token_address==(tokens['tether'])].reset_index(drop=True)\n",
    "        df['token_name']='tether'\n",
    "        # for k, v in tokens.items():\n",
    "        #     df.loc[df.token_address==v, 'token_name'] = k\n",
    "            \n",
    "        for v in ['to', 'from']:\n",
    "            df = df.merge(\n",
    "            addrs[['address']+feats], \n",
    "            left_on=f'{v}_address', right_on='address', \n",
    "            how=\"left\", validate='m:1'\n",
    "        ).rename(columns={c:f'{v}_{c}' for c in feats}).drop(columns=['address'])\n",
    "            \n",
    "        df['token_name'] = df.filter(like='token_name').bfill(axis=1).iloc[:, 0]\n",
    "        df = df.loc[:, ~df.columns.duplicated()].drop(columns=['from_address', 'to_address', 'token_address'])\n",
    "\n",
    "        df_ml = df.reset_index(drop=True)\n",
    "        df_ml = df_ml.loc[~(df_ml.from_name.isna() & df_ml.to_name.isna())]\n",
    "        df_ml.loc[df_ml.from_name.isna(), 'from_name'] = 'idk'\n",
    "        df_ml.loc[df_ml.to_name.isna(), 'to_name'] = 'idk'\n",
    "        \n",
    "        df_ml = df_ml\n",
    "        df_ml.to_parquet(\n",
    "            f'labelled/tron_martial_law_{year}_{month:02d}.parquet',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7853e344",
   "metadata": {},
   "source": [
    "# Refining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a11410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tron_martial_law_2025_04.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2025_04.parquet\n",
      "Processed tron_martial_law_2025_03.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2025_03.parquet\n",
      "Processed tron_martial_law_2025_02.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2025_02.parquet\n",
      "Processed tron_martial_law_2025_01.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2025_01.parquet\n",
      "Processed tron_martial_law_2024_12.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2024_12.parquet\n",
      "Processed tron_martial_law_2024_11.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2024_11.parquet\n",
      "Processed tron_martial_law_2020_01.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2020_01.parquet\n",
      "Processed tron_martial_law_2020_02.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2020_02.parquet\n",
      "Processed tron_martial_law_2020_03.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2020_03.parquet\n",
      "Processed tron_martial_law_2020_04.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2020_04.parquet\n",
      "Processed tron_martial_law_2020_05.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2020_05.parquet\n",
      "Processed tron_martial_law_2020_06.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2020_06.parquet\n",
      "Processed tron_martial_law_2020_07.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2020_07.parquet\n",
      "Processed tron_martial_law_2020_08.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2020_08.parquet\n",
      "Processed tron_martial_law_2020_09.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2020_09.parquet\n",
      "Processed tron_martial_law_2020_10.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2020_10.parquet\n",
      "Processed tron_martial_law_2020_11.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2020_11.parquet\n",
      "Processed tron_martial_law_2020_12.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2020_12.parquet\n",
      "Processed tron_martial_law_2021_01.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2021_01.parquet\n",
      "Processed tron_martial_law_2021_02.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2021_02.parquet\n",
      "Processed tron_martial_law_2021_03.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2021_03.parquet\n",
      "Processed tron_martial_law_2021_04.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2021_04.parquet\n",
      "Processed tron_martial_law_2021_05.parquet, saved to filtered_labelled\\tron\\tron_martial_law_2021_05.parquet\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def replace_long_with_idk(col):\n",
    "    mask = pc.greater(pc.utf8_length(pc.cast(col, pa.string())), 20)\n",
    "    return pc.if_else(mask, pa.array([\"idk\"] * len(col)), col)\n",
    "\n",
    "transformations = {\n",
    "    'transaction_hash': lambda col: pc.dictionary_encode(pc.cast(col, pa.string())).combine_chunks().indices\n",
    "}\n",
    "\n",
    "\n",
    "parquet_dir = 'labelled'\n",
    "col_to_replace = ['from_name', 'to_name']\n",
    "\n",
    "for filename in os.listdir(parquet_dir):\n",
    "    if not (filename.endswith('.parquet')):\n",
    "        continue\n",
    "    if 'tron' not in filename:\n",
    "        continue\n",
    "    \n",
    "    src_file_path = os.path.join(parquet_dir, filename)\n",
    "    dest_file_path = os.path.join(parquet_dir, filename.split('_')[0], filename)\n",
    "    # if os.path.exists('filtered_'+dest_file_path):\n",
    "    #    continue\n",
    "        \n",
    "    table = pq.read_table(src_file_path, columns=[\n",
    "        'block_timestamp', 'raw_amount', \n",
    "        'from_name', 'to_name', 'token_name',\n",
    "        'from_serviceType', 'from_sub1Service', 'from_walletPurpose', 'from_sub1Wallet',\n",
    "        'to_serviceType', 'to_sub1Service', 'to_walletPurpose', 'to_sub1Wallet',\n",
    "    ])\n",
    "    # for col_name in table.column_names:\n",
    "    #     null_count = pc.count(table[col_name], mode='only_null').as_py()\n",
    "    #     if null_count > 0:\n",
    "    #         print(f\"Column '{col_name}' has {null_count} null values\")\n",
    "        \n",
    "    # for c in col_to_replace:\n",
    "    #     table = table.set_column(table.schema.get_field_index(c), c, replace_long_with_idk(table[c]))\n",
    "\n",
    "    # filtered_table = table.filter(pc.invert(pc.and_(\n",
    "    #     pc.equal(table['from_name'], 'idk'),\n",
    "    #     pc.equal(table['to_name'], 'idk')\n",
    "    # )))\n",
    "    \n",
    "    filtered_table = table\n",
    "    \n",
    "\n",
    "    # for col_name, transform_func in transformations.items():\n",
    "    #     try:\n",
    "    #         filtered_table = filtered_table.set_column(\n",
    "    #             filtered_table.schema.get_field_index(col_name),\n",
    "    #             col_name,\n",
    "    #             transform_func(filtered_table[col_name])\n",
    "    #         )\n",
    "    #     except:\n",
    "    #         print(f\"Error transforming column {col_name} in {filename}\")\n",
    "    #         continue\n",
    "      \n",
    "    pa.parquet.write_table(filtered_table, 'filtered_' + dest_file_path)\n",
    "    print(f\"Processed {filename}, saved to filtered_{dest_file_path}\")\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50e91aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
